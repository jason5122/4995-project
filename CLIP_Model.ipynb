{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2owWIglXSb4",
        "outputId": "2fd971e0-f1fb-4ead-a85d-aae517d3fafd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-3w0pcvm6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-3w0pcvm6\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=bc985475ff5b2b6953e38fccb1be95b58778b4e77dcba6a0e6d12a4ba0d87664\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xzazmwad/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed clip-1.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "import clip"
      ],
      "metadata": {
        "id": "k5ErkHDaPKvH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Training on:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bazj7ESb7NYj",
        "outputId": "fbffc808-560f-43f6-b9cf-776511c0e7fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk9BYWh2H_2X",
        "outputId": "a4c19a53-08e3-490b-aa2b-c1527282a309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "DATA_FOLDER = '/content/drive/MyDrive/NNDL Project/Data'\n",
        "TRAIN_ZIP_PATH = os.path.join(DATA_FOLDER, 'train_images.zip')\n",
        "TEST_ZIP_PATH = os.path.join(DATA_FOLDER, 'test_images.zip')\n",
        "EXTRACT_PATH = '/content/data'\n",
        "\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(TRAIN_ZIP_PATH, 'r') as zip_ref:\n",
        "  zip_ref.extractall(EXTRACT_PATH)\n",
        "with zipfile.ZipFile(TEST_ZIP_PATH, 'r') as zip_ref:\n",
        "  zip_ref.extractall(EXTRACT_PATH)\n",
        "\n",
        "\n",
        "train_ann_df = pd.read_csv(os.path.join(DATA_FOLDER, 'train_data.csv'))\n",
        "super_map_df = pd.read_csv(os.path.join(DATA_FOLDER, 'superclass_mapping.csv'))\n",
        "sub_map_df = pd.read_csv(os.path.join(DATA_FOLDER, 'subclass_mapping.csv'))\n",
        "\n",
        "train_img_dir = os.path.join(EXTRACT_PATH, 'train_images')\n",
        "test_img_dir = os.path.join(EXTRACT_PATH, 'test_images')"
      ],
      "metadata": {
        "id": "DFN7tqXMJX09"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):  # Count files in img_dir\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ],
      "metadata": {
        "id": "awg6XpT1JijH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_preprocessing = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create train and val split\n",
        "train_dataset = MultiClassImageDataset(\n",
        "    train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing\n",
        ")\n",
        "train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1])\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = MultiClassImageTestDataset(\n",
        "    super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "4R8ZD4XBOeX8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jason's Experiments**"
      ],
      "metadata": {
        "id": "rbIrACCYN4EA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "    def train_epoch(self):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = (\n",
        "                data[0].to(device),\n",
        "                data[1].to(device),\n",
        "                data[3].to(device),\n",
        "            )\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(\n",
        "                sub_outputs, sub_labels\n",
        "            )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/i:.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, sub_labels = (\n",
        "                    data[0].to(device),\n",
        "                    data[1].to(device),\n",
        "                    data[3].to(device),\n",
        "                )\n",
        "\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                loss = self.criterion(super_outputs, super_labels) + self.criterion(\n",
        "                    sub_outputs, sub_labels\n",
        "                )\n",
        "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_predicted == super_labels).sum().item()\n",
        "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation loss: {running_loss/i:.3f}')\n",
        "        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
        "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %')\n",
        "\n",
        "    def test(self):\n",
        "      if not self.test_loader:\n",
        "          raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "      # Evaluate on test set, in this simple demo no special care is taken for novel/unseen classes\n",
        "      test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
        "      total_super_unseen = 0\n",
        "      total_sub_unseen = 0\n",
        "      with torch.no_grad():\n",
        "          for i, data in enumerate(self.test_loader):\n",
        "              inputs, img_names = data[0].to(device), data[1]\n",
        "              batch_size = inputs.size(0)\n",
        "\n",
        "              super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "              # We convert with softmax to apply the threshold to probabilities, not logits.\n",
        "              super_probs = F.softmax(super_outputs, dim=1)\n",
        "              sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "\n",
        "              super_max, super_pred = torch.max(super_probs.data, 1)\n",
        "              sub_max, sub_pred = torch.max(sub_probs.data, 1)\n",
        "\n",
        "              # Handle batched predictions\n",
        "              super_pred_labels = torch.where(super_max > 0.9, super_pred,\n",
        "                                            torch.ones_like(super_pred) * 3).cpu().numpy()\n",
        "              sub_pred_labels = torch.where(sub_max > 0.5, sub_pred,\n",
        "                                          torch.ones_like(sub_pred) * 87).cpu().numpy()\n",
        "\n",
        "              # Update statistics\n",
        "              total_super_unseen += (super_max <= 0.9).sum().item()\n",
        "              total_sub_unseen += (sub_max <= 0.5).sum().item()\n",
        "\n",
        "              # Add predictions to our results\n",
        "              for j in range(batch_size):\n",
        "                  test_predictions['image'].append(img_names[j])\n",
        "                  test_predictions['superclass_index'].append(int(super_pred_labels[j]))\n",
        "                  test_predictions['subclass_index'].append(int(sub_pred_labels[j]))\n",
        "\n",
        "      print(f'Total superclasses unseen: {total_super_unseen}')\n",
        "      print(f'Total subclasses unseen: {total_sub_unseen}')\n",
        "\n",
        "      return pd.DataFrame(data=test_predictions)"
      ],
      "metadata": {
        "id": "dMl8lZ6XOfQ8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPMultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, device, num_subclasses):\n",
        "        super().__init__()\n",
        "        self.clip_model, _ = clip.load('ViT-B/32', device=device)\n",
        "        self.clip_model.eval()\n",
        "        for param in self.clip_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.superclass_head = nn.Linear(self.clip_model.visual.output_dim, 4)\n",
        "        self.subclass_head = nn.Linear(self.clip_model.visual.output_dim, num_subclasses + 1)\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.clip_model.encode_image(images).float()\n",
        "\n",
        "        superclass_logits = self.superclass_head(features)\n",
        "        subclass_logits = self.subclass_head(features)\n",
        "        return superclass_logits, subclass_logits"
      ],
      "metadata": {
        "id": "g_Mzi805Ojme"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CLIPMultiLabelClassifier(device=device, num_subclasses=87).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader, device)\n",
        "\n",
        "for epoch in range(1):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    trainer.train_epoch()\n",
        "    trainer.validate_epoch()\n",
        "    print('')\n",
        "print('Finished training.')\n",
        "\n",
        "test_predictions = trainer.test()\n",
        "test_predictions.to_csv('test_predictions_clip.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bh8VbMxOkc5",
        "outputId": "ed9de7d8-2e78-40cf-87e2-6ffaa02be37f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:22<00:00, 15.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Training loss: 3.502\n",
            "Validation loss: 2.469\n",
            "Validation superclass acc: 99.84 %\n",
            "Validation subclass acc: 69.27 %\n",
            "\n",
            "Finished training.\n",
            "Total superclasses unseen: 4503\n",
            "Total subclasses unseen: 11180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Below are Nico's Experiments.***"
      ],
      "metadata": {
        "id": "fGz8oalB6by3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7lJRW-LxHAAE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# import torch.nn.functional as F # Not strictly needed in this version\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import clip\n",
        "from sklearn.model_selection import train_test_split # For stratified splitting\n",
        "import pandas as pd # Assuming train_ann_df and super_map_df are pandas DFs\n",
        "import numpy as np # For a few operations\n",
        "\n",
        "# --- 0. Configuration & Presumed Pre-loaded Data ---\n",
        "# Ensure these are defined before running this script:\n",
        "# train_ann_df = pd.read_csv(...) # Load your annotations\n",
        "# super_map_df = pd.read_csv(...) # Load your superclass mapping\n",
        "# train_img_dir = \"path/to/your/training/images\"\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Example placeholders if you want to run this snippet directly (replace with actual loading)\n",
        "if 'train_ann_df' not in globals():\n",
        "    print(\"Placeholder: Creating dummy train_ann_df and super_map_df\")\n",
        "    # Dummy super_map_df\n",
        "    super_map_data = {'index': [0, 1, 2], 'class': ['bird', 'reptile', 'dog']}\n",
        "    super_map_df = pd.DataFrame(super_map_data).set_index('index')\n",
        "\n",
        "    # Dummy train_ann_df\n",
        "    num_samples = 300\n",
        "    image_names = [f\"img_{i}.jpg\" for i in range(num_samples)]\n",
        "    superclass_indices = random.choices([0, 1, 2], k=num_samples) # 0:bird, 1:reptile, 2:dog\n",
        "    train_ann_df = pd.DataFrame({'image': image_names, 'superclass_index': superclass_indices})\n",
        "    # Create dummy image files if train_img_dir is also placeholder\n",
        "    # train_img_dir = \"./dummy_train_images\"\n",
        "    # if not os.path.exists(train_img_dir):\n",
        "    #     os.makedirs(train_img_dir)\n",
        "    #     for img_name in image_names:\n",
        "    #         try:\n",
        "    #             Image.new('RGB', (64,64)).save(os.path.join(train_img_dir, img_name))\n",
        "    #         except Exception as e:\n",
        "    #             print(f\"Could not create dummy image {img_name}: {e}\")\n",
        "\n",
        "\n",
        "DOG_CLASS_NAME = 'dog' # The class to be treated as \"novel\" in this phase\n",
        "\n",
        "# --- 1. Data Preparation & Splitting (Revised) ---\n",
        "\n",
        "# Find the original integer label for the class to be treated as novel\n",
        "try:\n",
        "    dog_original_label_idx = int(super_map_df[super_map_df['class'] == DOG_CLASS_NAME].index[0])\n",
        "except IndexError:\n",
        "    raise ValueError(f\"Class '{DOG_CLASS_NAME}' not found in super_map_df.\")\n",
        "except KeyError: # If 'index' is not the index name but a column\n",
        "    dog_original_label_idx = int(super_map_df[super_map_df['class'] == DOG_CLASS_NAME]['index'].iloc[0])\n",
        "\n",
        "\n",
        "# Identify all original labels and known original labels for this phase\n",
        "all_original_labels = sorted(train_ann_df['superclass_index'].unique())\n",
        "known_original_labels_for_phase1a = [l for l in all_original_labels if l != dog_original_label_idx]\n",
        "\n",
        "if not known_original_labels_for_phase1a:\n",
        "    raise ValueError(\"No 'known' classes remain after designating one as novel. Check your class setup.\")\n",
        "\n",
        "# Create a new contiguous mapping for labels for Phase 1a:\n",
        "# Known classes get 0, 1, ...\n",
        "# The \"novel\" proxy class (dog) gets the next available index.\n",
        "phase1a_label_map = {original_label: new_label for new_label, original_label in enumerate(known_original_labels_for_phase1a)}\n",
        "phase1a_novel_target_idx = len(known_original_labels_for_phase1a) # e.g., 2 if bird=0, reptile=1\n",
        "phase1a_label_map[dog_original_label_idx] = phase1a_novel_target_idx\n",
        "\n",
        "num_phase1a_outputs = phase1a_novel_target_idx + 1 # Total distinct target labels for this phase\n",
        "\n",
        "print(f\"Phase 1a: '{DOG_CLASS_NAME}' (original label {dog_original_label_idx}) will be mapped to 'novel' target label: {phase1a_novel_target_idx}\")\n",
        "print(f\"Other known classes mapped to: { {k:v for k,v in phase1a_label_map.items() if v != phase1a_novel_target_idx} }\")\n",
        "print(f\"Total output neurons for Phase 1a model: {num_phase1a_outputs}\")\n",
        "\n",
        "# Create a new column in train_ann_df for these Phase 1a target labels\n",
        "train_ann_df['phase1a_target_label'] = train_ann_df['superclass_index'].map(phase1a_label_map)\n",
        "\n",
        "# Split the main dataframe into training and validation sets\n",
        "# Stratify by the 'phase1a_target_label' to ensure both sets see all (remapped) classes\n",
        "try:\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        train_ann_df.index,\n",
        "        test_size=0.25,  # e.g., 25% for validation\n",
        "        stratify=train_ann_df['phase1a_target_label'],\n",
        "        random_state=42  # For reproducibility\n",
        "    )\n",
        "except ValueError as e:\n",
        "    print(f\"Warning: Stratified split failed with error: {e}. Falling back to non-stratified split.\")\n",
        "    print(\"This can happen if a class has too few samples for stratification.\")\n",
        "    # Check class counts for stratification\n",
        "    class_counts = train_ann_df['phase1a_target_label'].value_counts()\n",
        "    print(f\"Class counts for stratification: \\n{class_counts}\")\n",
        "    min_samples_for_stratify = 2 # sklearn usually needs at least 2 per class for stratification\n",
        "    if any(class_counts < min_samples_for_stratify):\n",
        "        print(f\"At least one class has fewer than {min_samples_for_stratify} samples, which causes stratification issues.\")\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        train_ann_df.index,\n",
        "        test_size=0.25,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "\n",
        "train_df = train_ann_df.loc[train_indices].reset_index(drop=True)\n",
        "val_df = train_ann_df.loc[val_indices].reset_index(drop=True)\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Train df 'phase1a_target_label' counts:\\n{train_df['phase1a_target_label'].value_counts()}\")\n",
        "print(f\"Val df 'phase1a_target_label' counts:\\n{val_df['phase1a_target_label'].value_counts()}\")\n",
        "\n",
        "\n",
        "# --- 2. Transforms (Targeting 224x224 for CLIP) ---\n",
        "CLIP_INPUT_SIZE = 224\n",
        "# CLIP's official normalization values\n",
        "CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
        "CLIP_STD = (0.26862954, 0.26130258, 0.27577711)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(CLIP_INPUT_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CLIP_MEAN, CLIP_STD),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((CLIP_INPUT_SIZE, CLIP_INPUT_SIZE)), # Resize without cropping for validation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CLIP_MEAN, CLIP_STD),\n",
        "])\n",
        "\n",
        "# --- 3. Updated Dataset Class for Phase 1a ---\n",
        "class SuperClassPhase1aDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_base_dir, transform_fn):\n",
        "        self.df = dataframe\n",
        "        self.img_base_dir = img_base_dir\n",
        "        self.transform_fn = transform_fn\n",
        "        # The 'phase1a_target_label' column in df already has the correct 0, 1, ... N-1 mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_base_dir, row['image'])\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"ERROR: Image not found at {img_path}\")\n",
        "            # Return a dummy image and label or raise error\n",
        "            # For now, creating a placeholder black image\n",
        "            img = Image.new('RGB', (CLIP_INPUT_SIZE, CLIP_INPUT_SIZE), color='black')\n",
        "            # It's better to ensure all images exist before this point\n",
        "        target_label = int(row['phase1a_target_label'])\n",
        "\n",
        "        if self.transform_fn:\n",
        "            img = self.transform_fn(img)\n",
        "\n",
        "        return img, target_label\n",
        "\n",
        "# --- 4. DataLoaders ---\n",
        "BATCH_SIZE = 32 # Reduced from 64 for potentially smaller datasets / memory constraints\n",
        "NUM_WORKERS = 2 # Adjusted from 4, common default\n",
        "\n",
        "train_ds = SuperClassPhase1aDataset(train_df, train_img_dir, train_transform)\n",
        "val_ds = SuperClassPhase1aDataset(val_df, train_img_dir, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True if device.type == 'cuda' else False)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True if device.type == 'cuda' else False)\n",
        "\n",
        "# --- 5. Model Definition (CLIP Backbone + Neck + Head for Phase 1a) ---\n",
        "def build_neck_super_phase1a(device_obj, num_phase1a_classes, neck_dim=512, drop_p=0.2): # Increased dropout slightly\n",
        "    class CLIPNeckSuperPhase1a(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            # Load CLIP model. It will be on the specified device.\n",
        "            self.clip_model, _ = clip.load('ViT-B/32', device=device_obj)\n",
        "            # Freeze CLIP parameters\n",
        "            for param in self.clip_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            clip_output_dim = self.clip_model.visual.output_dim\n",
        "            self.neck = nn.Sequential(\n",
        "                nn.Linear(clip_output_dim, neck_dim),\n",
        "                nn.LayerNorm(neck_dim), # LayerNorm is often good here\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(drop_p),\n",
        "                nn.Linear(neck_dim, clip_output_dim), # Project back or to another dimension\n",
        "            )\n",
        "            # Head for Phase 1a classes (e.g., bird, reptile, novel_dog_proxy)\n",
        "            self.super_head = nn.Linear(clip_output_dim, num_phase1a_classes)\n",
        "\n",
        "        def forward(self, images):\n",
        "            # CLIP model expects images directly, no need for torch.no_grad here if already frozen\n",
        "            # but if you want to be extra sure for the encoding part:\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip_model.encode_image(images).float()\n",
        "\n",
        "            neck_features = self.neck(image_features)\n",
        "            logits = self.super_head(neck_features)\n",
        "            return logits\n",
        "    # Instantiate and move to device\n",
        "    model = CLIPNeckSuperPhase1a().to(device_obj)\n",
        "    return model\n",
        "\n",
        "# --- 6. Model Instantiation, Criterion, Optimizer ---\n",
        "model = build_neck_super_phase1a(device, num_phase1a_outputs)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Only pass parameters of the neck and head to the optimizer\n",
        "optimizer = optim.Adam(\n",
        "    [param for param in model.neck.parameters() if param.requires_grad] +\n",
        "    [param for param in model.super_head.parameters() if param.requires_grad],\n",
        "    lr=1e-4, # Adjusted learning rate\n",
        "    weight_decay=1e-4 # Added weight decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler (optional, but good practice)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# --- 7. Training and Validation Loop ---\n",
        "NUM_EPOCHS = 1 # As originally planned for initial run\n",
        "\n",
        "print(f\"\\nStarting Phase 1a training for {NUM_EPOCHS} epochs...\")\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "    train_correct_preds = 0\n",
        "    train_total_samples = 0\n",
        "\n",
        "    for batch_idx, (images, target_labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        target_labels = target_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, target_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        preds = logits.argmax(dim=1)\n",
        "        train_correct_preds += (preds == target_labels).sum().item()\n",
        "        train_total_samples += target_labels.size(0)\n",
        "\n",
        "        if batch_idx % 50 == 0: # Print training progress every 50 batches\n",
        "             print(f\"  Epoch {epoch} [{(batch_idx+1)*BATCH_SIZE:>5}/{len(train_ds):>5} ({100.*(batch_idx+1)/len(train_loader):.0f}%)]\\tLoss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_accuracy = 100.0 * train_correct_preds / train_total_samples if train_total_samples > 0 else 0.0\n",
        "    print(f\"Epoch {epoch} Summary — Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    val_correct_all_preds = 0\n",
        "    val_correct_unseen_proxy_preds = 0\n",
        "    val_total_samples = 0\n",
        "    val_total_unseen_proxy_gt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, target_labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            target_labels_gpu = target_labels.to(device) # For loss and comparison with GPU preds\n",
        "\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, target_labels_gpu)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            preds_gpu = logits.argmax(dim=1)\n",
        "\n",
        "            val_total_samples += target_labels.size(0) # labels is on CPU here, .size(0) is fine\n",
        "            val_correct_all_preds += (preds_gpu == target_labels_gpu).sum().item()\n",
        "\n",
        "            # Identify unseen proxy samples (those whose target is phase1a_novel_target_idx)\n",
        "            # target_labels is on CPU, so use it for masking\n",
        "            unseen_proxy_mask_cpu = (target_labels == phase1a_novel_target_idx)\n",
        "            val_total_unseen_proxy_gt += unseen_proxy_mask_cpu.sum().item()\n",
        "\n",
        "            # Check predictions for these unseen proxy samples\n",
        "            # Apply the mask (moved to GPU) to the predictions (on GPU)\n",
        "            if unseen_proxy_mask_cpu.any(): # Ensure there are unseen samples in this batch\n",
        "                 val_correct_unseen_proxy_preds += (preds_gpu[unseen_proxy_mask_cpu.to(device)] == phase1a_novel_target_idx).sum().item()\n",
        "\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    overall_val_accuracy = 100.0 * val_correct_all_preds / val_total_samples if val_total_samples > 0 else 0.0\n",
        "    unseen_proxy_val_accuracy = 100.0 * val_correct_unseen_proxy_preds / val_total_unseen_proxy_gt if val_total_unseen_proxy_gt > 0 else 0.0 # Correct denominator\n",
        "\n",
        "    print(f\"  Validation — Avg Loss: {avg_val_loss:.4f}, Overall Acc: {overall_val_accuracy:.2f}%, \"\n",
        "          f\"Unseen ('{DOG_CLASS_NAME}' as novel) Acc: {unseen_proxy_val_accuracy:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Step the scheduler based on validation loss\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "print(\"Phase 1a training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJTFDPAKHA85",
        "outputId": "4805b107-6532-4b90-a8c7-f7da9d327fa5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 1a: 'dog' (original label 1) will be mapped to 'novel' target label: 2\n",
            "Other known classes mapped to: {np.int64(0): 0, np.int64(2): 1}\n",
            "Total output neurons for Phase 1a model: 3\n",
            "Training set size: 4716\n",
            "Validation set size: 1572\n",
            "Train df 'phase1a_target_label' counts:\n",
            "phase1a_target_label\n",
            "1    1766\n",
            "2    1563\n",
            "0    1387\n",
            "Name: count, dtype: int64\n",
            "Val df 'phase1a_target_label' counts:\n",
            "phase1a_target_label\n",
            "1    588\n",
            "2    521\n",
            "0    463\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Starting Phase 1a training for 1 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 [   32/ 4716 (1%)]\tLoss: 1.0863\n",
            "  Epoch 1 [ 1632/ 4716 (34%)]\tLoss: 0.0497\n",
            "  Epoch 1 [ 3232/ 4716 (68%)]\tLoss: 0.0105\n",
            "Epoch 1 Summary — Train Loss: 0.1250, Train Accuracy: 97.67%\n",
            "  Validation — Avg Loss: 0.0136, Overall Acc: 99.94%, Unseen ('dog' as novel) Acc: 100.00%\n",
            "--------------------------------------------------\n",
            "Phase 1a training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import clip\n",
        "\n",
        "# Assuming train_ann_df, super_map_df, train_img_dir, and device are already defined\n",
        "# from the previous Phase 1a code execution.\n",
        "# Also assuming val_ann_df was created during the train_test_split in Phase 1a.\n",
        "\n",
        "# Get the mapping of superclass names to their original integer indices\n",
        "superclass_to_index = dict(zip(super_map_df['class'], super_map_df.index))\n",
        "\n",
        "# --- transforms (now target 224x224 for CLIP) ---\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        (0.48145466, 0.4578275, 0.40821073),\n",
        "        (0.26862954, 0.26130258, 0.27577711)\n",
        "    ),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        (0.48145466, 0.4578275, 0.40821073),\n",
        "        (0.26862954, 0.26130258, 0.27577711)\n",
        "    ),\n",
        "])\n",
        "\n",
        "# --- Updated Dataset for super-class with 4 output neurons (including 'novel') ---\n",
        "class SuperClassDatasetPhase1b(Dataset):\n",
        "    def __init__(self, df, img_dir, transform):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.class_to_label = {\n",
        "            superclass_to_index['bird']: 0,\n",
        "            superclass_to_index['dog']: 1,\n",
        "            superclass_to_index['reptile']: 2,\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        img = Image.open(os.path.join(self.img_dir, row['image'])).convert('RGB')\n",
        "        original_label = int(row['superclass_index'])\n",
        "        target_label = self.class_to_label[original_label]  # Map to 0, 1, 2\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, target_label\n",
        "\n",
        "# Create DataLoaders for Phase 1b\n",
        "train_ds_phase1b = SuperClassDatasetPhase1b(train_df, train_img_dir, train_tf)\n",
        "val_ds_phase1b = SuperClassDatasetPhase1b(val_df, train_img_dir, val_tf) # Using the val_df created in Phase 1a\n",
        "\n",
        "train_loader_phase1b = DataLoader(train_ds_phase1b, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader_phase1b = DataLoader(val_ds_phase1b, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# --- build_neck with a 'novel' output neuron (4-way total) ---\n",
        "def build_neck_super_phase1b(device, neck_dim=512, drop_p=0.1):\n",
        "    class CLIPNeckSuperPhase1b(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.clip, _ = clip.load('ViT-B/32', device=device)\n",
        "            for p in self.clip.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "            D = self.clip.visual.output_dim\n",
        "            self.neck = nn.Sequential(\n",
        "                nn.Linear(D, neck_dim),\n",
        "                nn.LayerNorm(neck_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(drop_p),\n",
        "                nn.Linear(neck_dim, D),\n",
        "            )\n",
        "            # 4-way: bird, dog, reptile, novel\n",
        "            self.super_head = nn.Linear(D, 4)\n",
        "\n",
        "        def forward(self, x):\n",
        "            with torch.no_grad():\n",
        "                f = self.clip.encode_image(x).float()\n",
        "            f = self.neck(f)\n",
        "            return self.super_head(f) # logits over [bird, dog, reptile, novel]\n",
        "    return CLIPNeckSuperPhase1b().to(device)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_phase1b = build_neck_super_phase1b(device)\n",
        "criterion_phase1b = nn.CrossEntropyLoss()\n",
        "optimizer_phase1b = optim.Adam(filter(lambda p: p.requires_grad, model_phase1b.parameters()), lr=1e-3)\n",
        "\n",
        "# --- Training loop for 5 epochs (Phase 1b) ---\n",
        "for epoch in range(1, 10):\n",
        "    model_phase1b.train()\n",
        "    total_loss = 0\n",
        "    for imgs, labels in train_loader_phase1b:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer_phase1b.zero_grad()\n",
        "        logits = model_phase1b(imgs)\n",
        "        loss = criterion_phase1b(logits[:, :3], labels) # Only calculate loss on the first 3 outputs\n",
        "        loss.backward()\n",
        "        optimizer_phase1b.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch} — Train loss (Phase 1b): {total_loss/len(train_loader_phase1b):.3f}\")\n",
        "\n",
        "    # Validate (Phase 1b) with confidence thresholding for \"novel\"\n",
        "    model_phase1b.eval()\n",
        "    correct_known = total = novel_predictions = 0\n",
        "    confidence_threshold = 0.9 # You'll need to tune this\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, true_labels in val_loader_phase1b:\n",
        "            imgs = imgs.to(device)\n",
        "            logits = model_phase1b(imgs)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            max_prob_known = torch.max(probs[:, :3], dim=1).values\n",
        "            predicted_labels = torch.argmax(probs[:, :3], dim=1) # Predictions for known classes\n",
        "\n",
        "            # Identify as \"novel\" if confidence in known classes is low\n",
        "            novel_mask = (max_prob_known < confidence_threshold)\n",
        "            predicted_super_labels = torch.where(novel_mask, torch.tensor([3] * len(true_labels)).to(device), predicted_labels)\n",
        "\n",
        "            total += len(true_labels)\n",
        "            correct_known += (predicted_super_labels[~novel_mask].cpu() == true_labels[~novel_mask.cpu()]).sum().item()\n",
        "            novel_predictions += novel_mask.sum().item()\n",
        "\n",
        "    accuracy_known = 100 * correct_known / total if total > 0 else 0\n",
        "    novel_ratio = 100 * novel_predictions / total if total > 0 else 0\n",
        "    print(f\" Val Accuracy (Known Classes, Phase 1b): {accuracy_known:.2f}%\")\n",
        "    print(f\" Val Ratio Predicted as Novel (Threshold = {confidence_threshold}): {novel_ratio:.2f}%\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "print(\"Phase 1b training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8IruH8BHA_g",
        "outputId": "5c9d3321-9d78-4175-d41c-34b96684641f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 — Train loss (Phase 1b): 0.067\n",
            " Val Accuracy (Known Classes, Phase 1b): 99.75%\n",
            " Val Ratio Predicted as Novel (Threshold = 0.9): 0.19%\n",
            "--------------------------------------------------\n",
            "Epoch 2 — Train loss (Phase 1b): 0.012\n",
            " Val Accuracy (Known Classes, Phase 1b): 99.49%\n",
            " Val Ratio Predicted as Novel (Threshold = 0.9): 0.38%\n",
            "--------------------------------------------------\n",
            "Epoch 3 — Train loss (Phase 1b): 0.009\n",
            " Val Accuracy (Known Classes, Phase 1b): 98.92%\n",
            " Val Ratio Predicted as Novel (Threshold = 0.9): 0.95%\n",
            "--------------------------------------------------\n",
            "Epoch 4 — Train loss (Phase 1b): 0.009\n",
            " Val Accuracy (Known Classes, Phase 1b): 99.87%\n",
            " Val Ratio Predicted as Novel (Threshold = 0.9): 0.06%\n",
            "--------------------------------------------------\n",
            "Epoch 5 — Train loss (Phase 1b): 0.013\n",
            " Val Accuracy (Known Classes, Phase 1b): 99.62%\n",
            " Val Ratio Predicted as Novel (Threshold = 0.9): 0.32%\n",
            "--------------------------------------------------\n",
            "Epoch 6 — Train loss (Phase 1b): 0.010\n",
            " Val Accuracy (Known Classes, Phase 1b): 99.94%\n",
            " Val Ratio Predicted as Novel (Threshold = 0.9): 0.00%\n",
            "--------------------------------------------------\n",
            "Epoch 7 — Train loss (Phase 1b): 0.011\n",
            " Val Accuracy (Known Classes, Phase 1b): 99.75%\n",
            " Val Ratio Predicted as Novel (Threshold = 0.9): 0.13%\n",
            "--------------------------------------------------\n",
            "Epoch 8 — Train loss (Phase 1b): 0.006\n",
            " Val Accuracy (Known Classes, Phase 1b): 99.87%\n",
            " Val Ratio Predicted as Novel (Threshold = 0.9): 0.06%\n",
            "--------------------------------------------------\n",
            "Epoch 9 — Train loss (Phase 1b): 0.011\n",
            " Val Accuracy (Known Classes, Phase 1b): 99.55%\n",
            " Val Ratio Predicted as Novel (Threshold = 0.9): 0.32%\n",
            "--------------------------------------------------\n",
            "Phase 1b training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k1S305VUHBB2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the Dog Sub Classes"
      ],
      "metadata": {
        "id": "Kst0yVozo-no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming super_map_df and train_ann_df are already loaded\n",
        "\n",
        "DOG_SUPERCLASS_NAME = 'dog'\n",
        "\n",
        "# 1. Identify the super-class index for \"dog\"\n",
        "try:\n",
        "    dog_superclass_index = super_map_df[super_map_df['class'] == DOG_SUPERCLASS_NAME].index[0]\n",
        "except IndexError:\n",
        "    raise ValueError(f\"Super-class '{DOG_SUPERCLASS_NAME}' not found in super_map_df.\")\n",
        "\n",
        "print(f\"The super-class index for '{DOG_SUPERCLASS_NAME}' is: {dog_superclass_index}\")\n",
        "\n",
        "# 2. Filter train_ann_df for \"dog\" examples\n",
        "dog_train_df = train_ann_df[train_ann_df['superclass_index'] == dog_superclass_index].reset_index(drop=True)\n",
        "\n",
        "print(f\"Number of training examples for '{DOG_SUPERCLASS_NAME}': {len(dog_train_df)}\")\n",
        "\n",
        "# 3. Identify unique sub-class indices for \"dog\"\n",
        "dog_subclass_indices = dog_train_df['subclass_index'].unique()\n",
        "print(f\"Unique sub-class indices for '{DOG_SUPERCLASS_NAME}': {dog_subclass_indices}\")\n",
        "\n",
        "# 4. Map sub-class indices to names\n",
        "dog_subclasses = sub_map_df[sub_map_df.index.isin(dog_subclass_indices)]['class'].tolist()\n",
        "print(f\"Sub-classes for '{DOG_SUPERCLASS_NAME}': {dog_subclasses}\")\n",
        "\n",
        "num_dog_subclasses = len(dog_subclasses)\n",
        "print(f\"Number of sub-classes for '{DOG_SUPERCLASS_NAME}': {num_dog_subclasses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3cjp8LlHBEU",
        "outputId": "fb65a1f2-fc8f-4c91-b954-48dcd6e59c68"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The super-class index for 'dog' is: 1\n",
            "Number of training examples for 'dog': 2084\n",
            "Unique sub-class indices for 'dog': [37 62 31 70 65 64 36 49 45  7 22 17 46 18  2 85 21 10 79 54 38  0 53 23\n",
            " 32 25 12 77  9]\n",
            "Sub-classes for 'dog': ['Scotch terrier, Scottish terrier, Scottie', 'standard schnauzer', 'Pekinese, Pekingese, Peke', 'Lhasa, Lhasa apso', 'Lakeland terrier', 'Tibetan terrier, chrysanthemum dog', 'cairn, cairn terrier', 'Blenheim spaniel', 'Chihuahua', 'Japanese spaniel', 'Dandie Dinmont, Dandie Dinmont terrier', 'Airedale, Airedale terrier', 'Shih-Tzu', 'giant schnauzer', 'basset, basset hound', 'Maltese dog, Maltese terrier, Maltese', 'Sealyham terrier, Sealyham', 'Australian terrier', 'papillon', 'bloodhound, sleuthhound', 'soft-coated wheaten terrier', 'West Highland white terrier', 'Afghan hound, Afghan', 'Rhodesian ridgeback', 'beagle', 'toy terrier', 'silky terrier, Sydney silky', 'Boston bull, Boston terrier', 'miniature schnauzer']\n",
            "Number of sub-classes for 'dog': 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DogSubclassDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform, subclass_map_df):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.subclass_map_df = subclass_map_df\n",
        "        self.dog_superclass_index = 1  # From the previous output\n",
        "        self.dog_df = self.df[self.df['superclass_index'] == self.dog_superclass_index].reset_index(drop=True)\n",
        "        self.dog_subclass_indices = self.dog_df['subclass_index'].unique().tolist()\n",
        "        # Create a mapping from the original subclass index to a contiguous label (0 to num_dog_subclasses - 1)\n",
        "        self.subclass_to_label = {index: label for label, index in enumerate(sorted(self.dog_subclass_indices))}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dog_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dog_df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        subclass_index = row['subclass_index']\n",
        "        subclass_label = self.subclass_to_label[subclass_index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, subclass_label\n",
        "\n",
        "# Create the dataset for training the dog expert\n",
        "train_dog_dataset = DogSubclassDataset(train_ann_df, train_img_dir, image_preprocessing, sub_map_df)\n",
        "\n",
        "# Create a DataLoader for the dog training data\n",
        "batch_size = 64\n",
        "train_dog_loader = DataLoader(train_dog_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Let's also create a validation split for the dog data\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_len = int(0.8 * len(train_dog_dataset))\n",
        "val_len = len(train_dog_dataset) - train_len\n",
        "train_dog_dataset, val_dog_dataset = random_split(train_dog_dataset, [train_len, val_len])\n",
        "\n",
        "val_dog_loader = DataLoader(val_dog_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples for dog expert: {len(train_dog_dataset)}\")\n",
        "print(f\"Number of validation samples for dog expert: {len(val_dog_dataset)}\")\n",
        "print(f\"Number of dog sub-classes: {len(train_dog_dataset.dataset.subclass_to_label)}\")\n",
        "\n",
        "\n",
        "def build_dog_subclass_classifier(device, num_subclasses):\n",
        "    class CLIPNeckSubclass(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.clip, _ = clip.load('ViT-B/32', device=device)\n",
        "            for p in self.clip.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "            D = self.clip.visual.output_dim\n",
        "            self.neck = nn.Sequential(\n",
        "                nn.Linear(D, 512),\n",
        "                nn.LayerNorm(512),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(512, D),\n",
        "            )\n",
        "            # Output layer for the dog sub-classes + 1 for 'novel'\n",
        "            self.sub_head = nn.Linear(D, num_subclasses + 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            with torch.no_grad():\n",
        "                f = self.clip.encode_image(x).float()\n",
        "            f = self.neck(f)\n",
        "            return self.sub_head(f)\n",
        "\n",
        "    return CLIPNeckSubclass().to(device)\n",
        "\n",
        "num_dog_subclasses = len(train_dog_dataset.dataset.subclass_to_label)\n",
        "dog_expert_model = build_dog_subclass_classifier(device, num_dog_subclasses)\n",
        "criterion_dog = nn.CrossEntropyLoss()\n",
        "optimizer_dog = optim.Adam(filter(lambda p: p.requires_grad, dog_expert_model.parameters()), lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwpn6YbkHBGi",
        "outputId": "7af6b565-decc-4afc-e8bf-09ce47a8efde"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples for dog expert: 1667\n",
            "Number of validation samples for dog expert: 417\n",
            "Number of dog sub-classes: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dog_expert(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct_predictions / total_samples\n",
        "        print(f\"Epoch {epoch+1} — Train Loss (Dog Expert): {avg_loss:.3f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for val_imgs, val_labels in val_loader:\n",
        "                val_imgs, val_labels = val_imgs.to(device), val_labels.to(device)\n",
        "                val_logits = model(val_imgs)\n",
        "                loss = criterion(val_logits, val_labels)\n",
        "                val_loss += loss.item()\n",
        "                _, val_predicted = torch.max(val_logits, 1)\n",
        "                val_total += val_labels.size(0)\n",
        "                val_correct += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        print(f\"Epoch {epoch+1} — Val Loss (Dog Expert): {avg_val_loss:.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "# Train the dog expert\n",
        "train_dog_expert(dog_expert_model, train_dog_loader, val_dog_loader, criterion_dog, optimizer_dog, device, num_epochs=10)\n",
        "\n",
        "print(\"Dog expert training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k7baJssHBIo",
        "outputId": "f1df9cf7-7ec1-473e-c0c9-745c83eb0b69"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 — Train Loss (Dog Expert): 1.459, Train Accuracy: 60.36%\n",
            "Epoch 1 — Val Loss (Dog Expert): 0.392, Val Accuracy: 87.29%\n",
            "--------------------------------------------------\n",
            "Epoch 2 — Train Loss (Dog Expert): 0.304, Train Accuracy: 90.31%\n",
            "Epoch 2 — Val Loss (Dog Expert): 0.174, Val Accuracy: 94.96%\n",
            "--------------------------------------------------\n",
            "Epoch 3 — Train Loss (Dog Expert): 0.217, Train Accuracy: 92.13%\n",
            "Epoch 3 — Val Loss (Dog Expert): 0.234, Val Accuracy: 89.69%\n",
            "--------------------------------------------------\n",
            "Epoch 4 — Train Loss (Dog Expert): 0.191, Train Accuracy: 92.95%\n",
            "Epoch 4 — Val Loss (Dog Expert): 0.133, Val Accuracy: 95.68%\n",
            "--------------------------------------------------\n",
            "Epoch 5 — Train Loss (Dog Expert): 0.128, Train Accuracy: 95.44%\n",
            "Epoch 5 — Val Loss (Dog Expert): 0.137, Val Accuracy: 95.92%\n",
            "--------------------------------------------------\n",
            "Epoch 6 — Train Loss (Dog Expert): 0.109, Train Accuracy: 96.11%\n",
            "Epoch 6 — Val Loss (Dog Expert): 0.103, Val Accuracy: 97.12%\n",
            "--------------------------------------------------\n",
            "Epoch 7 — Train Loss (Dog Expert): 0.087, Train Accuracy: 96.69%\n",
            "Epoch 7 — Val Loss (Dog Expert): 0.105, Val Accuracy: 96.16%\n",
            "--------------------------------------------------\n",
            "Epoch 8 — Train Loss (Dog Expert): 0.070, Train Accuracy: 97.65%\n",
            "Epoch 8 — Val Loss (Dog Expert): 0.120, Val Accuracy: 94.96%\n",
            "--------------------------------------------------\n",
            "Epoch 9 — Train Loss (Dog Expert): 0.099, Train Accuracy: 96.26%\n",
            "Epoch 9 — Val Loss (Dog Expert): 0.068, Val Accuracy: 97.84%\n",
            "--------------------------------------------------\n",
            "Epoch 10 — Train Loss (Dog Expert): 0.050, Train Accuracy: 98.42%\n",
            "Epoch 10 — Val Loss (Dog Expert): 0.049, Val Accuracy: 98.32%\n",
            "--------------------------------------------------\n",
            "Dog expert training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the reptile sub classes**"
      ],
      "metadata": {
        "id": "lgZfh-CMpwl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming super_map_df and train_ann_df are already loaded\n",
        "\n",
        "REPTILE_SUPERCLASS_NAME = 'reptile'\n",
        "\n",
        "# 1. Identify the super-class index for \"reptile\"\n",
        "try:\n",
        "    reptile_superclass_index = super_map_df[super_map_df['class'] == REPTILE_SUPERCLASS_NAME].index[0]\n",
        "except IndexError:\n",
        "    raise ValueError(f\"Super-class '{REPTILE_SUPERCLASS_NAME}' not found in super_map_df.\")\n",
        "\n",
        "print(f\"The super-class index for '{REPTILE_SUPERCLASS_NAME}' is: {reptile_superclass_index}\")\n",
        "\n",
        "# 2. Filter train_ann_df for \"reptile\" examples\n",
        "reptile_train_df = train_ann_df[train_ann_df['superclass_index'] == reptile_superclass_index].reset_index(drop=True)\n",
        "\n",
        "print(f\"Number of training examples for '{REPTILE_SUPERCLASS_NAME}': {len(reptile_train_df)}\")\n",
        "\n",
        "# 3. Identify unique sub-class indices for \"reptile\"\n",
        "reptile_subclass_indices = reptile_train_df['subclass_index'].unique()\n",
        "print(f\"Unique sub-class indices for '{REPTILE_SUPERCLASS_NAME}': {reptile_subclass_indices}\")\n",
        "\n",
        "# 4. Map sub-class indices to names\n",
        "reptile_subclasses = sub_map_df[sub_map_df.index.isin(reptile_subclass_indices)]['class'].tolist()\n",
        "print(f\"Sub-classes for '{REPTILE_SUPERCLASS_NAME}': {reptile_subclasses}\")\n",
        "\n",
        "num_reptile_subclasses = len(reptile_subclasses)\n",
        "print(f\"Number of sub-classes for '{REPTILE_SUPERCLASS_NAME}': {num_reptile_subclasses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV8SF31AT-XR",
        "outputId": "76f74382-b14b-4308-c9dc-ce244b588717"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The super-class index for 'reptile' is: 2\n",
            "Number of training examples for 'reptile': 2354\n",
            "Unique sub-class indices for 'reptile': [61 57  3 50 47 81 66 52 55 76 71 48 29  1 67 63 34 44 35 74 58 72 69 43\n",
            " 13 15 68 39 33]\n",
            "Sub-classes for 'reptile': ['African chameleon, Chamaeleo chamaeleon', 'terrapin', 'agama', 'mud turtle', 'spotted salamander, Ambystoma maculatum', 'common iguana, iguana, Iguana iguana', 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui', 'thunder snake, worm snake, Carphophis amoenus', 'frilled lizard, Chlamydosaurus kingi', 'American alligator, Alligator mississipiensis', 'hognose snake, puff adder, sand viper', 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis', 'American chameleon, anole, Anolis carolinensis', 'European fire salamander, Salamandra salamandra', 'triceratops', 'loggerhead, loggerhead turtle, Caretta caretta', 'alligator lizard', 'African crocodile, Nile crocodile, Crocodylus niloticus', 'common newt, Triturus vulgaris', 'bullfrog, Rana catesbeiana', 'green lizard, Lacerta viridis', 'banded gecko', 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea', 'tree frog, tree-frog', 'Gila monster, Heloderma suspectum', 'axolotl, mud puppy, Ambystoma mexicanum', 'whiptail, whiptail lizard', 'green snake, grass snake', 'ringneck snake, ring-necked snake, ring snake']\n",
            "Number of sub-classes for 'reptile': 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReptileSubclassDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform, subclass_map_df):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.subclass_map_df = subclass_map_df\n",
        "        self.reptile_superclass_index = 2  # From the previous output\n",
        "        self.reptile_df = self.df[self.df['superclass_index'] == self.reptile_superclass_index].reset_index(drop=True)\n",
        "        self.reptile_subclass_indices = self.reptile_df['subclass_index'].unique().tolist()\n",
        "        # Create a mapping from the original subclass index to a contiguous label (0 to num_reptile_subclasses - 1)\n",
        "        self.subclass_to_label = {index: label for label, index in enumerate(sorted(self.reptile_subclass_indices))}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reptile_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.reptile_df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        subclass_index = row['subclass_index']\n",
        "        subclass_label = self.subclass_to_label[subclass_index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, subclass_label\n",
        "\n",
        "# Create the dataset for training the reptile expert\n",
        "train_reptile_dataset = ReptileSubclassDataset(train_ann_df, train_img_dir, image_preprocessing, sub_map_df)\n",
        "\n",
        "# Create a DataLoader for the reptile training data\n",
        "batch_size = 64\n",
        "train_reptile_loader = DataLoader(train_reptile_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Let's also create a validation split for the reptile data\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_len_reptile = int(0.8 * len(train_reptile_dataset))\n",
        "val_len_reptile = len(train_reptile_dataset) - train_len_reptile\n",
        "train_reptile_dataset, val_reptile_dataset = random_split(train_reptile_dataset, [train_len_reptile, val_len_reptile])\n",
        "\n",
        "val_reptile_loader = DataLoader(val_reptile_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples for reptile expert: {len(train_reptile_dataset)}\")\n",
        "print(f\"Number of validation samples for reptile expert: {len(val_reptile_dataset)}\")\n",
        "print(f\"Number of reptile sub-classes: {len(train_reptile_dataset.dataset.subclass_to_label)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXzd2Ts2T-Zo",
        "outputId": "83aae584-79eb-46f2-b656-a61d5d7e5336"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples for reptile expert: 1883\n",
            "Number of validation samples for reptile expert: 471\n",
            "Number of reptile sub-classes: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_reptile_subclass_classifier(device, num_subclasses):\n",
        "    class CLIPNeckSubclass(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.clip, _ = clip.load('ViT-B/32', device=device)\n",
        "            for p in self.clip.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "            D = self.clip.visual.output_dim\n",
        "            self.neck = nn.Sequential(\n",
        "                nn.Linear(D, 512),\n",
        "                nn.LayerNorm(512),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(512, D),\n",
        "            )\n",
        "            # Output layer for the reptile sub-classes + 1 for 'novel'\n",
        "            self.sub_head = nn.Linear(D, num_subclasses + 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            with torch.no_grad():\n",
        "                f = self.clip.encode_image(x).float()\n",
        "            f = self.neck(f)\n",
        "            return self.sub_head(f)\n",
        "\n",
        "    return CLIPNeckSubclass().to(device)\n",
        "\n",
        "num_reptile_subclasses = len(train_reptile_dataset.dataset.subclass_to_label)\n",
        "reptile_expert_model = build_reptile_subclass_classifier(device, num_reptile_subclasses)\n",
        "criterion_reptile = nn.CrossEntropyLoss()\n",
        "optimizer_reptile = optim.Adam(filter(lambda p: p.requires_grad, reptile_expert_model.parameters()), lr=1e-3)"
      ],
      "metadata": {
        "id": "pIXRwtWRT-bt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_reptile_expert(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct_predictions / total_samples\n",
        "        print(f\"Epoch {epoch+1} — Train Loss (Reptile Expert): {avg_loss:.3f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for val_imgs, val_labels in val_loader:\n",
        "                val_imgs, val_labels = val_imgs.to(device), val_labels.to(device)\n",
        "                val_logits = model(val_imgs)\n",
        "                loss = criterion(val_logits, val_labels)\n",
        "                val_loss += loss.item()\n",
        "                _, val_predicted = torch.max(val_logits, 1)\n",
        "                val_total += val_labels.size(0)\n",
        "                val_correct += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        print(f\"Epoch {epoch+1} — Val Loss (Reptile Expert): {avg_val_loss:.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "# Train the reptile expert\n",
        "train_reptile_expert(reptile_expert_model, train_reptile_loader, val_reptile_loader, criterion_reptile, optimizer_reptile, device, num_epochs=10)\n",
        "\n",
        "print(\"Reptile expert training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GANWoC8T-dx",
        "outputId": "80ef429a-f381-4f33-a890-6af3493e77bf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 — Train Loss (Reptile Expert): 1.341, Train Accuracy: 60.54%\n",
            "Epoch 1 — Val Loss (Reptile Expert): 0.376, Val Accuracy: 89.17%\n",
            "--------------------------------------------------\n",
            "Epoch 2 — Train Loss (Reptile Expert): 0.393, Train Accuracy: 86.19%\n",
            "Epoch 2 — Val Loss (Reptile Expert): 0.260, Val Accuracy: 91.08%\n",
            "--------------------------------------------------\n",
            "Epoch 3 — Train Loss (Reptile Expert): 0.264, Train Accuracy: 90.31%\n",
            "Epoch 3 — Val Loss (Reptile Expert): 0.217, Val Accuracy: 92.36%\n",
            "--------------------------------------------------\n",
            "Epoch 4 — Train Loss (Reptile Expert): 0.227, Train Accuracy: 91.76%\n",
            "Epoch 4 — Val Loss (Reptile Expert): 0.141, Val Accuracy: 95.54%\n",
            "--------------------------------------------------\n",
            "Epoch 5 — Train Loss (Reptile Expert): 0.177, Train Accuracy: 93.50%\n",
            "Epoch 5 — Val Loss (Reptile Expert): 0.091, Val Accuracy: 97.88%\n",
            "--------------------------------------------------\n",
            "Epoch 6 — Train Loss (Reptile Expert): 0.138, Train Accuracy: 95.20%\n",
            "Epoch 6 — Val Loss (Reptile Expert): 0.159, Val Accuracy: 95.33%\n",
            "--------------------------------------------------\n",
            "Epoch 7 — Train Loss (Reptile Expert): 0.109, Train Accuracy: 95.96%\n",
            "Epoch 7 — Val Loss (Reptile Expert): 0.063, Val Accuracy: 97.45%\n",
            "--------------------------------------------------\n",
            "Epoch 8 — Train Loss (Reptile Expert): 0.118, Train Accuracy: 95.62%\n",
            "Epoch 8 — Val Loss (Reptile Expert): 0.171, Val Accuracy: 93.63%\n",
            "--------------------------------------------------\n",
            "Epoch 9 — Train Loss (Reptile Expert): 0.141, Train Accuracy: 94.90%\n",
            "Epoch 9 — Val Loss (Reptile Expert): 0.077, Val Accuracy: 96.18%\n",
            "--------------------------------------------------\n",
            "Epoch 10 — Train Loss (Reptile Expert): 0.078, Train Accuracy: 97.41%\n",
            "Epoch 10 — Val Loss (Reptile Expert): 0.062, Val Accuracy: 97.24%\n",
            "--------------------------------------------------\n",
            "Reptile expert training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the reptile sub classes**"
      ],
      "metadata": {
        "id": "oif8viXbrVsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming super_map_df and train_ann_df are already loaded\n",
        "\n",
        "BIRD_SUPERCLASS_NAME = 'bird'\n",
        "\n",
        "# 1. Identify the super-class index for \"bird\"\n",
        "try:\n",
        "    bird_superclass_index = super_map_df[super_map_df['class'] == BIRD_SUPERCLASS_NAME].index[0]\n",
        "except IndexError:\n",
        "    raise ValueError(f\"Super-class '{BIRD_SUPERCLASS_NAME}' not found in super_map_df.\")\n",
        "\n",
        "print(f\"The super-class index for '{BIRD_SUPERCLASS_NAME}' is: {bird_superclass_index}\")\n",
        "\n",
        "# 2. Filter train_ann_df for \"bird\" examples\n",
        "bird_train_df = train_ann_df[train_ann_df['superclass_index'] == bird_superclass_index].reset_index(drop=True)\n",
        "\n",
        "print(f\"Number of training examples for '{BIRD_SUPERCLASS_NAME}': {len(bird_train_df)}\")\n",
        "\n",
        "# 3. Identify unique sub-class indices for \"bird\"\n",
        "bird_subclass_indices = bird_train_df['subclass_index'].unique()\n",
        "print(f\"Unique sub-class indices for '{BIRD_SUPERCLASS_NAME}': {bird_subclass_indices}\")\n",
        "\n",
        "# 4. Map sub-class indices to names\n",
        "bird_subclasses = sub_map_df[sub_map_df.index.isin(bird_subclass_indices)]['class'].tolist()\n",
        "print(f\"Sub-classes for '{BIRD_SUPERCLASS_NAME}': {bird_subclasses}\")\n",
        "\n",
        "num_bird_subclasses = len(bird_subclasses)\n",
        "print(f\"Number of sub-classes for '{BIRD_SUPERCLASS_NAME}': {num_bird_subclasses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7msVed6CT-f-",
        "outputId": "74b750ee-38fb-43d7-a84e-afb90bdad1b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The super-class index for 'bird' is: 0\n",
            "Number of training examples for 'bird': 1850\n",
            "Unique sub-class indices for 'bird': [42  4 41 20 19 60 78 16  6 40 28 30 84 59 75 24  8 80 86 27 14 82 51 56\n",
            " 26  5 11 73 83]\n",
            "Sub-classes for 'bird': ['great grey owl, great gray owl, Strix nebulosa', 'bustard', 'ptarmigan', 'hen', 'pelican', 'junco, snowbird', 'cock', 'brambling, Fringilla montifringilla', 'king penguin, Aptenodytes patagonica', 'bald eagle, American eagle, Haliaeetus leucocephalus', 'albatross, mollymawk', 'water ouzel, dipper', 'black grouse', 'vulture', 'red-backed sandpiper, dunlin, Erolia alpina', 'redshank, Tringa totanus', 'oystercatcher, oyster catcher', 'ostrich, Struthio camelus', 'bulbul', 'house finch, linnet, Carpodacus mexicanus', 'goldfinch, Carduelis carduelis', 'dowitcher', 'chickadee', 'indigo bunting, indigo finch, indigo bird, Passerina cyanea', 'American coot, marsh hen, mud hen, water hen, Fulica americana', 'ruddy turnstone, Arenaria interpres', 'robin, American robin, Turdus migratorius', 'magpie', 'European gallinule, Porphyrio porphyrio']\n",
            "Number of sub-classes for 'bird': 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BirdSubclassDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform, subclass_map_df):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.subclass_map_df = subclass_map_df\n",
        "        self.bird_superclass_index = 0  # From the previous output\n",
        "        self.bird_df = self.df[self.df['superclass_index'] == self.bird_superclass_index].reset_index(drop=True)\n",
        "        self.bird_subclass_indices = self.bird_df['subclass_index'].unique().tolist()\n",
        "        # Create a mapping from the original subclass index to a contiguous label (0 to num_bird_subclasses - 1)\n",
        "        self.subclass_to_label = {index: label for label, index in enumerate(sorted(self.bird_subclass_indices))}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.bird_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.bird_df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        subclass_index = row['subclass_index']\n",
        "        subclass_label = self.subclass_to_label[subclass_index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, subclass_label\n",
        "\n",
        "# Create the dataset for training the bird expert\n",
        "train_bird_dataset = BirdSubclassDataset(train_ann_df, train_img_dir, image_preprocessing, sub_map_df)\n",
        "\n",
        "# Create a DataLoader for the bird training data\n",
        "batch_size = 64\n",
        "train_bird_loader = DataLoader(train_bird_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Let's also create a validation split for the bird data\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_len_bird = int(0.8 * len(train_bird_dataset))\n",
        "val_len_bird = len(train_bird_dataset) - train_len_bird\n",
        "train_bird_dataset, val_bird_dataset = random_split(train_bird_dataset, [train_len_bird, val_len_bird])\n",
        "\n",
        "val_bird_loader = DataLoader(val_bird_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples for bird expert: {len(train_bird_dataset)}\")\n",
        "print(f\"Number of validation samples for bird expert: {len(val_bird_dataset)}\")\n",
        "print(f\"Number of bird sub-classes: {len(train_bird_dataset.dataset.subclass_to_label)}\")\n",
        "\n",
        "def build_bird_subclass_classifier(device, num_subclasses):\n",
        "    class CLIPNeckSubclass(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.clip, _ = clip.load('ViT-B/32', device=device)\n",
        "            for p in self.clip.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "            D = self.clip.visual.output_dim\n",
        "            self.neck = nn.Sequential(\n",
        "                nn.Linear(D, 512),\n",
        "                nn.LayerNorm(512),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(512, D),\n",
        "            )\n",
        "            # Output layer for the bird sub-classes + 1 for 'novel'\n",
        "            self.sub_head = nn.Linear(D, num_subclasses + 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            with torch.no_grad():\n",
        "                f = self.clip.encode_image(x).float()\n",
        "            f = self.neck(f)\n",
        "            return self.sub_head(f)\n",
        "\n",
        "    return CLIPNeckSubclass().to(device)\n",
        "\n",
        "num_bird_subclasses = len(train_bird_dataset.dataset.subclass_to_label)\n",
        "bird_expert_model = build_bird_subclass_classifier(device, num_bird_subclasses)\n",
        "criterion_bird = nn.CrossEntropyLoss()\n",
        "optimizer_bird = optim.Adam(filter(lambda p: p.requires_grad, bird_expert_model.parameters()), lr=1e-3)\n",
        "\n",
        "def train_bird_expert(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct_predictions / total_samples\n",
        "        print(f\"Epoch {epoch+1} — Train Loss (Bird Expert): {avg_loss:.3f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for val_imgs, val_labels in val_loader:\n",
        "                val_imgs, val_labels = val_imgs.to(device), val_labels.to(device)\n",
        "                val_logits = model(val_imgs)\n",
        "                loss = criterion(val_logits, val_labels)\n",
        "                val_loss += loss.item()\n",
        "                _, val_predicted = torch.max(val_logits, 1)\n",
        "                val_total += val_labels.size(0)\n",
        "                val_correct += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        print(f\"Epoch {epoch+1} — Val Loss (Bird Expert): {avg_val_loss:.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "# Train the bird expert\n",
        "train_bird_expert(bird_expert_model, train_bird_loader, val_bird_loader, criterion_bird, optimizer_bird, device, num_epochs=10)\n",
        "\n",
        "print(\"Bird expert training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jebuOtXQrXsX",
        "outputId": "9d3398d4-1620-4221-9518-c72c13a30737"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples for bird expert: 1480\n",
            "Number of validation samples for bird expert: 370\n",
            "Number of bird sub-classes: 29\n",
            "Epoch 1 — Train Loss (Bird Expert): 1.121, Train Accuracy: 74.54%\n",
            "Epoch 1 — Val Loss (Bird Expert): 0.118, Val Accuracy: 97.84%\n",
            "--------------------------------------------------\n",
            "Epoch 2 — Train Loss (Bird Expert): 0.099, Train Accuracy: 97.19%\n",
            "Epoch 2 — Val Loss (Bird Expert): 0.055, Val Accuracy: 98.65%\n",
            "--------------------------------------------------\n",
            "Epoch 3 — Train Loss (Bird Expert): 0.072, Train Accuracy: 97.41%\n",
            "Epoch 3 — Val Loss (Bird Expert): 0.045, Val Accuracy: 98.92%\n",
            "--------------------------------------------------\n",
            "Epoch 4 — Train Loss (Bird Expert): 0.028, Train Accuracy: 99.41%\n",
            "Epoch 4 — Val Loss (Bird Expert): 0.013, Val Accuracy: 99.73%\n",
            "--------------------------------------------------\n",
            "Epoch 5 — Train Loss (Bird Expert): 0.013, Train Accuracy: 99.73%\n",
            "Epoch 5 — Val Loss (Bird Expert): 0.007, Val Accuracy: 100.00%\n",
            "--------------------------------------------------\n",
            "Epoch 6 — Train Loss (Bird Expert): 0.010, Train Accuracy: 99.84%\n",
            "Epoch 6 — Val Loss (Bird Expert): 0.003, Val Accuracy: 100.00%\n",
            "--------------------------------------------------\n",
            "Epoch 7 — Train Loss (Bird Expert): 0.005, Train Accuracy: 100.00%\n",
            "Epoch 7 — Val Loss (Bird Expert): 0.002, Val Accuracy: 100.00%\n",
            "--------------------------------------------------\n",
            "Epoch 8 — Train Loss (Bird Expert): 0.004, Train Accuracy: 100.00%\n",
            "Epoch 8 — Val Loss (Bird Expert): 0.001, Val Accuracy: 100.00%\n",
            "--------------------------------------------------\n",
            "Epoch 9 — Train Loss (Bird Expert): 0.002, Train Accuracy: 99.95%\n",
            "Epoch 9 — Val Loss (Bird Expert): 0.002, Val Accuracy: 100.00%\n",
            "--------------------------------------------------\n",
            "Epoch 10 — Train Loss (Bird Expert): 0.021, Train Accuracy: 99.46%\n",
            "Epoch 10 — Val Loss (Bird Expert): 0.042, Val Accuracy: 97.57%\n",
            "--------------------------------------------------\n",
            "Bird expert training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalValidationDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform, superclass_to_index):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.superclass_to_index = superclass_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        superclass_index = int(row['superclass_index'])\n",
        "        subclass_index = int(row['subclass_index'])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, superclass_index, subclass_index"
      ],
      "metadata": {
        "id": "AvXNmNoJrsz8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds_hierarchical = HierarchicalValidationDataset(val_df, train_img_dir, val_tf, superclass_to_index)\n",
        "val_loader_hierarchical = DataLoader(val_ds_hierarchical, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "U6qupW6i4gb7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def validate_hierarchical(super_class_model, dog_expert, reptile_expert, bird_expert, val_loader, device, super_map_df, sub_map_df, train_dog_dataset, train_reptile_dataset, train_bird_dataset, confidence_threshold_superclass=0.8, confidence_threshold_subclass=0.7):\n",
        "    \"\"\"\n",
        "    Validates the hierarchical classifier on the validation dataset with sub-class labels.\n",
        "\n",
        "    Args:\n",
        "        super_class_model (nn.Module): Trained super-class classifier.\n",
        "        dog_expert (nn.Module): Trained dog sub-class expert.\n",
        "        reptile_expert (nn.Module): Trained reptile sub-class expert.\n",
        "        bird_expert (nn.Module): Trained bird sub-class expert.\n",
        "        val_loader (DataLoader): DataLoader for the validation set (super-class AND sub-class labels).\n",
        "        device (str): 'cuda' or 'cpu'.\n",
        "        super_map_df (pd.DataFrame): DataFrame mapping super-class indices to names.\n",
        "        sub_map_df (pd.DataFrame): DataFrame mapping sub-class indices to names.\n",
        "        train_dog_dataset (Dataset): Training dataset for the dog expert (to access label mapping).\n",
        "        train_reptile_dataset (Dataset): Training dataset for the reptile expert (to access label mapping).\n",
        "        train_bird_dataset (Dataset): Training dataset for the bird expert (to access label mapping).\n",
        "        confidence_threshold_superclass (float): Threshold for super-class novelty.\n",
        "        confidence_threshold_subclass (float): Threshold for sub-class novelty.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of validation metrics (super-class accuracy, sub-class accuracy, etc.).\n",
        "    \"\"\"\n",
        "    super_class_model.eval()\n",
        "    dog_expert.eval()\n",
        "    reptile_expert.eval()\n",
        "    bird_expert.eval()\n",
        "\n",
        "    correct_super = 0\n",
        "    total_super = 0\n",
        "    novel_super_predicted = 0\n",
        "    novel_super_actual = 0\n",
        "\n",
        "    correct_sub = 0\n",
        "    total_sub = 0\n",
        "    novel_sub_predicted = 0\n",
        "    novel_sub_actual = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, super_labels, sub_labels in val_loader: # Unpack sub_labels\n",
        "            imgs = imgs.to(device)\n",
        "            super_labels = super_labels.to(device)\n",
        "            sub_labels = sub_labels.to(device)\n",
        "\n",
        "            # 1. Super-class prediction and evaluation\n",
        "            super_logits = super_class_model(imgs)\n",
        "            super_probs = torch.softmax(super_logits, dim=1)\n",
        "            max_prob_superclass, predicted_superclass_index = torch.max(super_probs[:, :3], dim=1)\n",
        "            predicted_super_list = [super_map_df.iloc[i]['class'] for i in predicted_superclass_index.cpu().numpy()]\n",
        "            true_super_indices = super_labels.cpu().numpy()\n",
        "            true_super_list = [super_map_df.iloc[i]['class'] for i in true_super_indices]\n",
        "            is_novel_superclass = (max_prob_superclass < confidence_threshold_superclass).cpu().numpy()\n",
        "\n",
        "            for i in range(len(super_labels)):\n",
        "                if is_novel_superclass[i]:\n",
        "                    if true_super_list[i] not in ['bird', 'dog', 'reptile']:\n",
        "                        novel_super_actual += 1\n",
        "                    novel_super_predicted += 1\n",
        "                else:\n",
        "                    if predicted_super_list[i] == true_super_list[i]:\n",
        "                        correct_super += 1\n",
        "                total_super += 1\n",
        "\n",
        "            # 2. Sub-class prediction and evaluation\n",
        "            for i in range(len(imgs)):\n",
        "                img = imgs[i].unsqueeze(0)\n",
        "                predicted_superclass = predicted_super_list[i]\n",
        "                true_superclass = true_super_list[i]\n",
        "                true_subclass_index = sub_labels[i].item()\n",
        "                true_subclass_name = sub_map_df.iloc[true_subclass_index]['class']\n",
        "\n",
        "                sub_logits = None\n",
        "                index_to_subclass = None\n",
        "                predicted_subclass_name = \"novel\"\n",
        "                max_prob_subclass_val = 0.0\n",
        "\n",
        "                if predicted_superclass == 'dog':\n",
        "                    sub_logits = dog_expert(img)\n",
        "                    index_to_subclass = {v: k for k, v in train_dog_dataset.dataset.subclass_to_label.items()}\n",
        "                elif predicted_superclass == 'reptile':\n",
        "                    sub_logits = reptile_expert(img)\n",
        "                    index_to_subclass = {v: k for k, v in train_reptile_dataset.dataset.subclass_to_label.items()}\n",
        "                elif predicted_superclass == 'bird':\n",
        "                    sub_logits = bird_expert(img)\n",
        "                    index_to_subclass = {v: k for k, v in train_bird_dataset.dataset.subclass_to_label.items()}\n",
        "\n",
        "                if sub_logits is not None:\n",
        "                    sub_probs = torch.softmax(sub_logits[:, :-1], dim=1)\n",
        "                    max_prob_subclass, predicted_subclass_index_local = torch.max(sub_probs, dim=1)\n",
        "                    max_prob_subclass_val = max_prob_subclass.item()\n",
        "\n",
        "                    if max_prob_subclass_val >= confidence_threshold_subclass:\n",
        "                        # FIXED: Use the values directly instead of sorted keys positions\n",
        "                        original_subclass_indices = list(index_to_subclass.values())\n",
        "                        original_subclass_index = original_subclass_indices[predicted_subclass_index_local.item()]\n",
        "                        predicted_subclass_name = sub_map_df.iloc[original_subclass_index]['class']\n",
        "\n",
        "                    if predicted_superclass == true_superclass:\n",
        "                        total_sub += 1\n",
        "                        if predicted_subclass_name == true_subclass_name:\n",
        "                            correct_sub += 1\n",
        "                        # --- Conceptual Novel Sub-class Detection (Needs Careful Definition) ---\n",
        "                        # if max_prob_subclass_val < confidence_threshold_subclass and true_subclass_name is a novel sub-class:\n",
        "                        #     novel_sub_actual += 1\n",
        "                        # if predicted_subclass_name == \"novel\" and (true_subclass_name is a novel sub-class or max_prob_subclass_val < confidence_threshold_subclass):\n",
        "                        #     novel_sub_predicted += 1\n",
        "\n",
        "    super_accuracy = 100 * correct_super / total_super if total_super > 0 else 0\n",
        "    novel_super_precision = (novel_super_predicted / (novel_super_predicted + (total_super - correct_super - novel_super_predicted))) * 100 if novel_super_predicted > 0 else 0\n",
        "    novel_super_recall = (novel_super_predicted / novel_super_actual) * 100 if novel_super_actual > 0 else 0\n",
        "\n",
        "    sub_accuracy = 100 * correct_sub / total_sub if total_sub > 0 else 0\n",
        "    # novel_sub_precision = (novel_sub_predicted / (novel_sub_predicted + (total_sub - correct_sub - novel_sub_predicted))) * 100 if novel_sub_predicted > 0 else 0\n",
        "    # novel_sub_recall = (novel_sub_predicted / novel_sub_actual) * 100 if novel_sub_actual > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"super_class_accuracy\": super_accuracy,\n",
        "        \"novel_super_precision\": novel_super_precision,\n",
        "        \"novel_super_recall\": novel_super_recall,\n",
        "        \"sub_class_accuracy\": sub_accuracy,\n",
        "        # \"novel_sub_precision\": novel_sub_precision,\n",
        "        # \"novel_sub_recall\": novel_sub_recall,\n",
        "    }\n",
        "\n",
        "# --- Run Validation with Sub-class Labels ---\n",
        "validation_metrics = validate_hierarchical(\n",
        "    model_phase1b,\n",
        "    dog_expert_model,\n",
        "    reptile_expert_model,\n",
        "    bird_expert_model,\n",
        "    val_loader_hierarchical, # Use the new DataLoader\n",
        "    device,\n",
        "    super_map_df,\n",
        "    sub_map_df,\n",
        "    train_dog_dataset,\n",
        "    train_reptile_dataset,\n",
        "    train_bird_dataset\n",
        ")\n",
        "\n",
        "print(\"Hierarchical Classifier Validation Metrics (with Sub-class Evaluation):\")\n",
        "for metric, value in validation_metrics.items():\n",
        "    print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhiws6JPrs2b",
        "outputId": "c1b8f5bf-328e-49d4-952c-258f02bd0f22"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hierarchical Classifier Validation Metrics (with Sub-class Evaluation):\n",
            "super_class_accuracy: 99.61832061068702\n",
            "novel_super_precision: 66.66666666666666\n",
            "novel_super_recall: 0\n",
            "sub_class_accuracy: 96.29865985960434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: what is the list of unique superclass_index values in val_df and what is the list of unique subclass_index values in val_df?\n",
        "\n",
        "print(f\"Unique superclass_index values in val_df: {val_df['superclass_index'].unique().tolist()}\")\n",
        "print(f\"Unique subclass_index values in val_df: {val_df['subclass_index'].unique().tolist()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi2V2-JEyf_l",
        "outputId": "5def79af-1b8c-4622-e206-446bff99b978"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique superclass_index values in val_df: [2, 1, 0]\n",
            "Unique subclass_index values in val_df: [39, 49, 42, 4, 13, 54, 38, 66, 61, 7, 74, 32, 18, 11, 45, 29, 65, 48, 30, 23, 70, 6, 68, 57, 71, 76, 21, 9, 83, 19, 28, 15, 81, 77, 0, 86, 84, 24, 44, 22, 80, 62, 16, 52, 41, 17, 69, 43, 35, 3, 64, 47, 37, 1, 46, 50, 14, 12, 27, 31, 2, 36, 72, 75, 26, 67, 59, 56, 58, 40, 55, 79, 78, 85, 63, 51, 73, 60, 8, 25, 20, 53, 10, 5, 82, 34, 33]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build wrapper class for compatibility with trainer**"
      ],
      "metadata": {
        "id": "wkoWCXL5Cg5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import clip\n",
        "\n",
        "class HierarchicalCLIPClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper model that combines superclass classifier and expert subclass classifiers\n",
        "    into a single model interface compatible with the Trainer class.\n",
        "    \"\"\"\n",
        "    def __init__(self, device, superclass_model, dog_expert, reptile_expert, bird_expert,\n",
        "                 superclass_threshold=0.9, subclass_threshold=0.9):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.superclass_model = superclass_model\n",
        "        self.dog_expert = dog_expert\n",
        "        self.reptile_expert = reptile_expert\n",
        "        self.bird_expert = bird_expert\n",
        "\n",
        "        self.superclass_threshold = superclass_threshold\n",
        "        self.subclass_threshold = subclass_threshold\n",
        "\n",
        "        # Maps superclass index to name\n",
        "        self.superclass_idx_to_name = {0: 'bird', 1: 'dog', 2: 'reptile'}\n",
        "        # Maps superclass name to expert model\n",
        "        self.expert_models = {\n",
        "            'bird': self.bird_expert,\n",
        "            'dog': self.dog_expert,\n",
        "            'reptile': self.reptile_expert\n",
        "        }\n",
        "\n",
        "        # Maps from expert outputs to original subclass indices (prepare during initialization)\n",
        "        self.expert_mappings = {\n",
        "            'bird': list(train_bird_dataset.dataset.subclass_to_label.values()),\n",
        "            'dog': list(train_dog_dataset.dataset.subclass_to_label.values()),\n",
        "            'reptile': list(train_reptile_dataset.dataset.subclass_to_label.values())\n",
        "        }\n",
        "\n",
        "        # Novel class indices\n",
        "        self.NOVEL_SUPERCLASS_IDX = 3  # Index for novel superclass\n",
        "        self.NOVEL_SUBCLASS_IDX = 87   # Index for novel subclass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get superclass predictions\n",
        "        superclass_logits = self.superclass_model(x)\n",
        "\n",
        "        # Prepare subclass logits tensor (batch_size x num_subclasses)\n",
        "        # Initialize with very negative values (will be ignored in softmax)\n",
        "        batch_size = x.size(0)\n",
        "        subclass_logits = torch.ones(batch_size, 88, device=self.device) * -100\n",
        "\n",
        "        # Get superclass predictions to route to experts\n",
        "        super_probs = F.softmax(superclass_logits, dim=1)\n",
        "        max_prob_superclass, predicted_superclass_idx = torch.max(super_probs[:, :3], dim=1)\n",
        "\n",
        "        # Process each sample in batch individually to route to appropriate expert\n",
        "        for i in range(batch_size):\n",
        "            # Skip if prediction confidence is below threshold (mark as novel)\n",
        "            if max_prob_superclass[i] < self.superclass_threshold:\n",
        "                subclass_logits[i, self.NOVEL_SUBCLASS_IDX] = 0  # Set novel class logit to 0 (others are -100)\n",
        "                continue\n",
        "\n",
        "            # Get predicted superclass and corresponding expert\n",
        "            pred_superclass_idx = predicted_superclass_idx[i].item()\n",
        "            pred_superclass_name = self.superclass_idx_to_name[pred_superclass_idx]\n",
        "            expert_model = self.expert_models[pred_superclass_name]\n",
        "\n",
        "            # Get expert predictions\n",
        "            expert_output = expert_model(x[i:i+1])  # Process single sample\n",
        "            sub_probs = F.softmax(expert_output[:, :-1], dim=1)  # Exclude novel output\n",
        "            max_prob_subclass, pred_subclass_local_idx = torch.max(sub_probs, dim=1)\n",
        "\n",
        "            # If confidence is below threshold, mark as novel\n",
        "            if max_prob_subclass.item() < self.subclass_threshold:\n",
        "                subclass_logits[i, self.NOVEL_SUBCLASS_IDX] = 0\n",
        "                continue\n",
        "\n",
        "            # Map to original subclass index\n",
        "            original_subclass_indices = self.expert_mappings[pred_superclass_name]\n",
        "            original_subclass_idx = original_subclass_indices[pred_subclass_local_idx.item()]\n",
        "\n",
        "            # Set the corresponding logit to a high value (will dominate in softmax)\n",
        "            subclass_logits[i, original_subclass_idx] = 10.0\n",
        "\n",
        "        return superclass_logits, subclass_logits"
      ],
      "metadata": {
        "id": "3z4XQKWVrs46"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Initialize the hierarchical model\n",
        "hierarchical_model = HierarchicalCLIPClassifier(\n",
        "    device=device,\n",
        "    superclass_model=model_phase1b,\n",
        "    dog_expert=dog_expert_model,\n",
        "    reptile_expert=reptile_expert_model,\n",
        "    bird_expert=bird_expert_model,\n",
        "    superclass_threshold=0.7,  # Adjust as needed\n",
        "    subclass_threshold=0.7     # Adjust as needed\n",
        ")\n",
        "\n",
        "# Set up training components\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Only optimize the parameters that need training if models are already trained\n",
        "optimizer = optim.Adam(hierarchical_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Create the trainer\n",
        "trainer = Trainer(\n",
        "    model=hierarchical_model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Skip training if models are already trained\n",
        "# for epoch in range(1):\n",
        "#     print(f'Epoch {epoch+1}')\n",
        "#     trainer.train_epoch()\n",
        "#     trainer.validate_epoch()\n",
        "#     print('')\n",
        "\n",
        "# Generate test predictions\n",
        "print('Generating test predictions...')\n",
        "test_predictions = trainer.test()\n",
        "test_predictions.to_csv('test_predictions_hierarchical.csv', index=False)\n",
        "print('Test predictions saved to test_predictions_hierarchical.csv')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0c-W95VArs7U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3f3c91f7-9ae4-4db5-f4f7-50c6fbd97e6a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Initialize the hierarchical model\\nhierarchical_model = HierarchicalCLIPClassifier(\\n    device=device,\\n    superclass_model=model_phase1b,\\n    dog_expert=dog_expert_model,\\n    reptile_expert=reptile_expert_model,\\n    bird_expert=bird_expert_model,\\n    superclass_threshold=0.7,  # Adjust as needed\\n    subclass_threshold=0.7     # Adjust as needed\\n)\\n\\n# Set up training components\\ncriterion = nn.CrossEntropyLoss()\\n# Only optimize the parameters that need training if models are already trained\\noptimizer = optim.Adam(hierarchical_model.parameters(), lr=1e-3)\\n\\n# Create the trainer\\ntrainer = Trainer(\\n    model=hierarchical_model,\\n    criterion=criterion,\\n    optimizer=optimizer,\\n    train_loader=train_loader,\\n    val_loader=val_loader,\\n    test_loader=test_loader,\\n    device=device\\n)\\n\\n# Skip training if models are already trained\\n# for epoch in range(1):\\n#     print(f'Epoch {epoch+1}')\\n#     trainer.train_epoch()\\n#     trainer.validate_epoch()\\n#     print('')\\n\\n# Generate test predictions\\nprint('Generating test predictions...')\\ntest_predictions = trainer.test()\\ntest_predictions.to_csv('test_predictions_hierarchical.csv', index=False)\\nprint('Test predictions saved to test_predictions_hierarchical.csv')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the hierarchical model\n",
        "hierarchical_model = HierarchicalCLIPClassifier(\n",
        "    device=device,\n",
        "    superclass_model=model_phase1b,\n",
        "    dog_expert=dog_expert_model,\n",
        "    reptile_expert=reptile_expert_model,\n",
        "    bird_expert=bird_expert_model,\n",
        "    superclass_threshold=0.65,  # Adjusted threshold\n",
        "    subclass_threshold=0.5      # Adjusted threshold\n",
        ")\n",
        "\n",
        "# Set up training components\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(hierarchical_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Create the trainer\n",
        "trainer = Trainer(\n",
        "    model=hierarchical_model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Custom validation function for hierarchical model on val_loader_hierarchical\n",
        "def validate_with_hierarchical(model, val_loader_hierarchical, device):\n",
        "    model.eval()\n",
        "    correct_super = 0\n",
        "    total_super = 0\n",
        "    correct_sub = 0\n",
        "    total_sub = 0\n",
        "    novel_super_pred = 0\n",
        "    novel_sub_pred = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, super_labels, sub_labels in val_loader_hierarchical:\n",
        "            imgs = imgs.to(device)\n",
        "            super_labels = super_labels.to(device)\n",
        "            sub_labels = sub_labels.to(device)\n",
        "\n",
        "            super_logits, sub_logits = model(imgs)\n",
        "\n",
        "            # Handle superclass predictions\n",
        "            super_probs = F.softmax(super_logits, dim=1)\n",
        "            max_prob_super, pred_super = torch.max(super_probs[:, :3], dim=1)\n",
        "            novel_super_mask = max_prob_super < model.superclass_threshold\n",
        "            pred_super[novel_super_mask] = 3  # Set to novel class\n",
        "\n",
        "            # Calculate superclass accuracy\n",
        "            total_super += super_labels.size(0)\n",
        "            correct_super += (pred_super == super_labels).sum().item()\n",
        "            novel_super_pred += novel_super_mask.sum().item()\n",
        "\n",
        "            # Handle subclass predictions and accuracy\n",
        "            sub_probs = F.softmax(sub_logits, dim=1)\n",
        "            max_prob_sub, pred_sub = torch.max(sub_probs, dim=1)\n",
        "            novel_sub_mask = pred_sub == 87  # Identify novel subclass predictions\n",
        "\n",
        "            # Only count subclass accuracy for samples with correct superclass\n",
        "            correct_super_mask = (pred_super == super_labels)\n",
        "            total_sub += correct_super_mask.sum().item()\n",
        "            correct_sub_and_super = correct_super_mask & (pred_sub == sub_labels)\n",
        "            correct_sub += correct_sub_and_super.sum().item()\n",
        "            novel_sub_pred += novel_sub_mask.sum().item()\n",
        "\n",
        "    # Calculate final metrics\n",
        "    super_acc = 100 * correct_super / total_super if total_super > 0 else 0\n",
        "    sub_acc = 100 * correct_sub / total_sub if total_sub > 0 else 0\n",
        "    novel_super_percent = 100 * novel_super_pred / total_super if total_super > 0 else 0\n",
        "    novel_sub_percent = 100 * novel_sub_pred / total_super if total_super > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"Superclass Accuracy\": super_acc,\n",
        "        \"Subclass Accuracy\": sub_acc,\n",
        "        \"Novel Superclass %\": novel_super_percent,\n",
        "        \"Novel Subclass %\": novel_sub_percent\n",
        "    }\n",
        "\n",
        "# Run validation on hierarchical validation set\n",
        "print(\"Validating hierarchical model on validation set...\")\n",
        "val_metrics = validate_with_hierarchical(hierarchical_model, val_loader_hierarchical, device)\n",
        "print(\"Validation Metrics:\")\n",
        "for metric, value in val_metrics.items():\n",
        "    print(f\"{metric}: {value:.2f}%\")\n",
        "\n",
        "# Generate test predictions\n",
        "print('\\nGenerating test predictions...')\n",
        "test_predictions = trainer.test()\n",
        "test_predictions.to_csv('test_predictions_hierarchical.csv', index=False)\n",
        "print('Test predictions saved to test_predictions_hierarchical.csv')\n",
        "\n",
        "# Analyze test predictions\n",
        "print(\"\\nTest Predictions Distribution:\")\n",
        "print(f\"Total test samples: {len(test_predictions)}\")\n",
        "print(f\"Superclass distribution:\\n{test_predictions['superclass_index'].value_counts()}\")\n",
        "print(f\"Percentage of novel superclass predictions: {(test_predictions['superclass_index'] == 3).mean() * 100:.2f}%\")\n",
        "print(f\"Percentage of novel subclass predictions: {(test_predictions['subclass_index'] == 87).mean() * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCtB91KTkIjN",
        "outputId": "3f9d6ad6-28bf-47e1-ef2a-b1dcb74ead1f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating hierarchical model on validation set...\n",
            "Validation Metrics:\n",
            "Superclass Accuracy: 99.62%\n",
            "Subclass Accuracy: 0.70%\n",
            "Novel Superclass %: 0.19%\n",
            "Novel Subclass %: 0.64%\n",
            "\n",
            "Generating test predictions...\n",
            "Total superclasses unseen: 217\n",
            "Total subclasses unseen: 0\n",
            "Test predictions saved to test_predictions_hierarchical.csv\n",
            "\n",
            "Test Predictions Distribution:\n",
            "Total test samples: 11180\n",
            "Superclass distribution:\n",
            "superclass_index\n",
            "2    4563\n",
            "0    3412\n",
            "1    2988\n",
            "3     217\n",
            "Name: count, dtype: int64\n",
            "Percentage of novel superclass predictions: 1.94%\n",
            "Percentage of novel subclass predictions: 10.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Now, let's run the test and generate predictions\n",
        "print(\"\\nRunning test predictions...\")\n",
        "test_predictions = trainer.test()\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "test_predictions.to_csv('test_predictions_hierarchical.csv', index=False)\n",
        "print(\"Test predictions saved to test_predictions_hierarchical.csv\")\n",
        "\n",
        "# Let's examine the test predictions distribution\n",
        "print(\"\\nTest Predictions Distribution:\")\n",
        "print(f\"Total test samples: {len(test_predictions)}\")\n",
        "print(f\"Superclass distribution:\\n{test_predictions['superclass_index'].value_counts()}\")\n",
        "print(f\"Percentage of novel superclass predictions: {(test_predictions['superclass_index'] == 3).mean() * 100:.2f}%\")\n",
        "print(f\"Percentage of novel subclass predictions: {(test_predictions['subclass_index'] == 87).mean() * 100:.2f}%\")\n",
        "\n",
        "# Check if our predictions match the format expected by the leaderboard\n",
        "print(\"\\nValidating prediction format:\")\n",
        "print(f\"All superclass indices are within expected range (0-3): {test_predictions['superclass_index'].between(0, 3).all()}\")\n",
        "print(f\"All subclass indices are within expected range (0-87): {test_predictions['subclass_index'].between(0, 87).all()}\")\n",
        "print(f\"No missing values in predictions: {not test_predictions.isnull().any().any()}\")\n",
        "\n",
        "# Show a sample of the predictions\n",
        "print(\"\\nSample of test predictions:\")\n",
        "print(test_predictions.head(10))"
      ],
      "metadata": {
        "id": "9JRix5HQrs92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35ac4a1-f872-4bf7-b126-03f5ed14e04a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running test predictions...\n",
            "Total superclasses unseen: 217\n",
            "Total subclasses unseen: 0\n",
            "Test predictions saved to test_predictions_hierarchical.csv\n",
            "\n",
            "Test Predictions Distribution:\n",
            "Total test samples: 11180\n",
            "Superclass distribution:\n",
            "superclass_index\n",
            "2    4563\n",
            "0    3412\n",
            "1    2988\n",
            "3     217\n",
            "Name: count, dtype: int64\n",
            "Percentage of novel superclass predictions: 1.94%\n",
            "Percentage of novel subclass predictions: 10.75%\n",
            "\n",
            "Validating prediction format:\n",
            "All superclass indices are within expected range (0-3): True\n",
            "All subclass indices are within expected range (0-87): True\n",
            "No missing values in predictions: True\n",
            "\n",
            "Sample of test predictions:\n",
            "   image  superclass_index  subclass_index\n",
            "0  0.jpg                 1              17\n",
            "1  1.jpg                 0               6\n",
            "2  2.jpg                 2               6\n",
            "3  3.jpg                 2              15\n",
            "4  4.jpg                 1              13\n",
            "5  5.jpg                 2               3\n",
            "6  6.jpg                 0               4\n",
            "7  7.jpg                 2               9\n",
            "8  8.jpg                 0              25\n",
            "9  9.jpg                 2              22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and analyze the predictions\n",
        "test_predictions = pd.read_csv('test_predictions_hierarchical.csv')\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"Total test samples: {len(test_predictions)}\")\n",
        "print(f\"Superclass distribution:\")\n",
        "print(test_predictions['superclass_index'].value_counts())\n",
        "print(f\"Percentage classified as novel superclass: {(test_predictions['superclass_index'] == 3).mean() * 100:.2f}%\")\n",
        "\n",
        "# Look at subclass distribution\n",
        "print(f\"\\nSubclass distribution - top 10 most common:\")\n",
        "print(test_predictions['subclass_index'].value_counts().head(10))\n",
        "print(f\"Number of unique subclasses predicted: {test_predictions['subclass_index'].nunique()}\")\n",
        "\n",
        "# Check for patterns in superclass-subclass relationship\n",
        "print(\"\\nMost common subclasses for each superclass:\")\n",
        "for super_idx in range(4):  # 0, 1, 2, 3 (where 3 is novel)\n",
        "    super_mask = test_predictions['superclass_index'] == super_idx\n",
        "    if super_mask.sum() > 0:\n",
        "        print(f\"Superclass {super_idx}:\")\n",
        "        print(test_predictions.loc[super_mask, 'subclass_index'].value_counts().head(5))"
      ],
      "metadata": {
        "id": "Z5R_kZ8NrtBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca34e76-2721-40f4-ece6-33c674413efd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total test samples: 11180\n",
            "Superclass distribution:\n",
            "superclass_index\n",
            "2    4563\n",
            "0    3412\n",
            "1    2988\n",
            "3     217\n",
            "Name: count, dtype: int64\n",
            "Percentage classified as novel superclass: 1.94%\n",
            "\n",
            "Subclass distribution - top 10 most common:\n",
            "subclass_index\n",
            "87    1202\n",
            "23     985\n",
            "22     846\n",
            "15     820\n",
            "19     619\n",
            "8      526\n",
            "6      509\n",
            "13     459\n",
            "10     433\n",
            "18     425\n",
            "Name: count, dtype: int64\n",
            "Number of unique subclasses predicted: 30\n",
            "\n",
            "Most common subclasses for each superclass:\n",
            "Superclass 0:\n",
            "subclass_index\n",
            "6     363\n",
            "23    312\n",
            "87    307\n",
            "4     303\n",
            "13    286\n",
            "Name: count, dtype: int64\n",
            "Superclass 1:\n",
            "subclass_index\n",
            "19    418\n",
            "18    366\n",
            "87    242\n",
            "23    209\n",
            "2     207\n",
            "Name: count, dtype: int64\n",
            "Superclass 2:\n",
            "subclass_index\n",
            "22    786\n",
            "15    658\n",
            "87    556\n",
            "23    443\n",
            "8     266\n",
            "Name: count, dtype: int64\n",
            "Superclass 3:\n",
            "subclass_index\n",
            "87    97\n",
            "22    31\n",
            "15    26\n",
            "23    21\n",
            "25    10\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHq71khHrXxk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7BPONqpzT-iN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gv30CPzbHBNT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1OYeCF3I51Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UW9PA-oaI533"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gvr6IuY_I56N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mFZGdFlvI58r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nudJ4oBiHBPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGwfyrghHBR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJkujK3IHBUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "iTjwHgbtYIYv",
        "outputId": "4c371c4a-53a4-4f87-89df-affc9b15fb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running experiment: baseline ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ca24dbecab79>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_subclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m87\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ca24dbecab79>\u001b[0m in \u001b[0;36mbuild_baseline\u001b[0;34m(device, num_subclasses)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_subclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# exactly your existing CLIPMultiLabelClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLIPMultiLabelClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_subclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_subclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-132cbf9c43e2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, device, num_subclasses)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_subclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ViT-B/32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/clip/clip.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, device, jit, download_root)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# loading JIT archive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mjit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/jit/_serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, _extra_files, _restore_shapes)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mcpp_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_ir_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_extra_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_restore_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         cpp_module = torch._C.import_ir_module_from_buffer(\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mcu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_extra_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_restore_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         )  # type: ignore[call-arg]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LqfNmRjc6mqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jJ55okFC_fTP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}